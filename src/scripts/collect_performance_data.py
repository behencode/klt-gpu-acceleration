#!/usr/bin/env python3
"""
Benchmark collection utility for the KLT GPU acceleration project.

This script builds each version (V1–V4) for CPU and GPU targets, runs them
against every dataset (small/medium/large), and aggregates all timing data
needed for the reporting graphs.  All metrics are derived from the live runs
— no hand-written values.
"""

from __future__ import annotations

import argparse
import json
import os
import statistics
import subprocess
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple
import resource


DatasetName = str
VersionName = str


# Files generated by the trackers that must be cleaned between runs
INTERMEDIATE_PATTERNS = [
    "feat*.ppm",
    "features.*",
    "gmon.out",
    "profile.txt",
]


@dataclass
class RunSample:
    elapsed: float
    user: float
    sys: float
    returncode: int
    stdout: str
    stderr: str

    @property
    def other(self) -> float:
        return max(self.elapsed - self.user - self.sys, 0.0)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Collect benchmark data for CPU/GPU versions of KLT."
    )
    default_root = Path(__file__).resolve().parents[1]
    parser.add_argument(
        "--src-root",
        type=Path,
        default=default_root,
        help="Path to the src/ directory (defaults to script parent).",
    )
    parser.add_argument(
        "--dataset-root",
        type=Path,
        default=Path("dataset"),
        help="Relative path (within src-root) to datasets.",
    )
    parser.add_argument(
        "--versions",
        nargs="+",
        default=["V1", "V2", "V3", "V4"],
        help="Versions to benchmark.",
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=3,
        help="Iterations per version/device/dataset combination.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("performance_data.json"),
        help="Destination JSON file for the collected data.",
    )
    parser.add_argument(
        "--reference-dataset",
        default="large",
        help="Dataset name to use for GPU-focused graphs.",
    )
    parser.add_argument(
        "--strong-scaling-blocks",
        nargs="+",
        type=int,
        default=[64, 128, 256, 512, 1024],
        help="Thread/block sizes to probe for the strong scaling study.",
    )
    parser.add_argument(
        "--profile-versions",
        nargs="+",
        default=["V1", "V4"],
        help="Versions to profile with gprof for the hotspot analysis.",
    )
    parser.add_argument(
        "--strong-scaling-iterations",
        type=int,
        default=2,
        help="Iterations per configuration for the strong scaling study.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print planned actions without running benchmarks.",
    )
    return parser.parse_args()


def log(msg: str) -> None:
    now = datetime.now(timezone.utc).isoformat()
    print(f"[collect {now}] {msg}")


def read_pgm_metadata(pgm_path: Path) -> Tuple[int, int, int]:
    # PGM files store width, height, and bit depth in ASCII headers. We only need
    # those values to estimate the total number of pixels/bytes in a dataset.
    with pgm_path.open("rb") as fh:
        magic = fh.readline().strip()
        if magic not in {b"P5", b"P2"}:
            raise ValueError(f"Unsupported PGM format {magic!r} in {pgm_path}")
        width = height = maxval = None
        while True:
            line = fh.readline()
            if not line:
                break
            line = line.strip()
            if not line or line.startswith(b"#"):
                continue
            parts = line.split()
            if width is None:
                if len(parts) != 2:
                    raise ValueError(f"Malformed size line {line!r}")
                width, height = map(int, parts)
                continue
            maxval = int(parts[0])
            break
    if width is None or height is None or maxval is None:
        raise ValueError(f"Failed to parse metadata from {pgm_path}")
    return width, height, maxval


def discover_datasets(dataset_root: Path) -> Dict[DatasetName, Path]:
    # Dataset directories contain per-frame .pgm files. Each top-level folder
    # counts as one dataset (small/medium/large, etc.).
    datasets = {}
    if not dataset_root.exists():
        raise FileNotFoundError(f"Dataset root {dataset_root} not found.")
    for entry in sorted(dataset_root.iterdir()):
        if entry.is_dir():
            datasets[entry.name] = entry
    if not datasets:
        raise RuntimeError(f"No datasets discovered under {dataset_root}")
    return datasets


def cleanup_dataset_outputs(dataset_path: Path, skip: Optional[Sequence[str]] = None) -> None:
    # Every tracker run dumps intermediate feature files into the dataset folder.
    # Remove them before/after runs so measurements never pick up stale data.
    skip = set(skip or ())
    for pattern in INTERMEDIATE_PATTERNS:
        if pattern in skip:
            continue
        for candidate in dataset_path.glob(pattern):
            try:
                candidate.unlink()
            except FileNotFoundError:
                pass


def run_command(
    cmd: Sequence[str],
    cwd: Path,
    env: Optional[Dict[str, str]] = None,
) -> subprocess.CompletedProcess:
    proc = subprocess.run(
        list(cmd),
        cwd=str(cwd),
        env=env,
        capture_output=True,
        text=True,
    )
    return proc


def build_target(version_dir: Path, target: str) -> Tuple[bool, str]:
    proc = run_command(["make", target], version_dir)
    success = proc.returncode == 0
    if not success:
        log(f"[warn] make {target} in {version_dir.name} failed: {proc.stderr.strip()}")
    return success, proc.stderr + proc.stdout


def trim_output(text: str, limit: int = 2000) -> str:
    if len(text) <= limit:
        return text
    return text[: limit - 3] + "..."


def execute_binary(
    binary: Path,
    dataset_path: Path,
    env: Optional[Dict[str, str]] = None,
    cleanup_after_skip: Optional[Sequence[str]] = None,
) -> RunSample:
    # Clear out dataset artifacts to make room for the next run.
    env_vars = os_environ_with_updates(env)
    cleanup_dataset_outputs(dataset_path)
    # Use resource.getrusage so we can separate user/sys/other time later.
    start = time.perf_counter()
    usage_before = resource.getrusage(resource.RUSAGE_CHILDREN)
    proc = run_command([str(binary)], dataset_path, env_vars)
    end = time.perf_counter()
    usage_after = resource.getrusage(resource.RUSAGE_CHILDREN)
    sample = RunSample(
        elapsed=end - start,
        user=max(usage_after.ru_utime - usage_before.ru_utime, 0.0),
        sys=max(usage_after.ru_stime - usage_before.ru_stime, 0.0),
        returncode=proc.returncode,
        stdout=trim_output(proc.stdout),
        stderr=trim_output(proc.stderr),
    )
    cleanup_dataset_outputs(dataset_path, skip=cleanup_after_skip)
    return sample


def os_environ_with_updates(extra: Optional[Dict[str, str]]) -> Dict[str, str]:
    env = os.environ.copy()
    if extra:
        env.update(extra)
    return env


def collect_runs(
    binary: Path,
    dataset_path: Path,
    iterations: int,
    env: Optional[Dict[str, str]] = None,
) -> Dict:
    # Run the same binary multiple times to build a statistical sample. Fail-fast
    # if any iteration returns non-zero so we capture the stderr in the output.
    samples: List[RunSample] = []
    failure_reason: Optional[str] = None
    for i in range(iterations):
        sample = execute_binary(binary, dataset_path, env)
        samples.append(sample)
        if sample.returncode != 0:
            failure_reason = (
                f"Iteration {i} failed with code {sample.returncode}: {sample.stderr}"
            )
            break
    stats = summarize_samples(samples)
    return {
        "binary": str(binary),
        "iterations": len(samples),
        "samples": [sample.__dict__ for sample in samples],
        "stats": stats,
        "error": failure_reason,
    }


def summarize_samples(samples: Sequence[RunSample]) -> Dict[str, Optional[float]]:
    # Turn the raw RunSample objects into aggregate stats. These numbers feed
    # directly into the JSON report and the later visualization scripts.
    if not samples:
        return {"mean": None, "median": None, "stdev": None, "user": None, "sys": None, "other": None}
    elapsed = [s.elapsed for s in samples]
    user = [s.user for s in samples]
    sys_times = [s.sys for s in samples]
    other = [s.other for s in samples]
    def safe_stat(fn, values):
        if len(values) >= 2:
            return fn(values)
        return values[0] if values else None
    summary = {
        "mean": statistics.mean(elapsed),
        "median": statistics.median(elapsed),
        "stdev": statistics.stdev(elapsed) if len(elapsed) >= 2 else 0.0,
        "user": statistics.mean(user),
        "sys": statistics.mean(sys_times),
        "other": statistics.mean(other),
    }
    return summary


def best_stat(stats: Dict[str, Optional[float]]) -> Optional[float]:
    if not stats:
        return None
    return stats.get("mean")


def collect_hotspots(
    version_dir: Path,
    dataset_path: Path,
    top_n: int = 8,
) -> List[Tuple[str, float]]:
    # Build the gprof-enabled binary, run it once, then parse "flat profile" to
    # extract the hottest functions. We only keep the top N percentages.
    build_ok, _ = build_target(version_dir, "example3_prof")
    if not build_ok:
        return []
    binary = version_dir / "example3_prof"
    sample = execute_binary(binary, dataset_path, cleanup_after_skip=("gmon.out",))
    if sample.returncode != 0:
        log(
            f"[warn] profiling run for {version_dir.name} failed: {sample.stderr}"
        )
        return []
    gmon = dataset_path / "gmon.out"
    if not gmon.exists():
        log(f"[warn] profiling output missing for {version_dir.name}")
        return []
    proc = run_command(["gprof", str(binary), str(gmon.name)], dataset_path)
    try:
        gmon.unlink()
    except FileNotFoundError:
        pass
    if proc.returncode != 0:
        log(f"[warn] gprof failed for {version_dir.name}: {proc.stderr}")
        return []
    return parse_gprof(proc.stdout, top_n)


def parse_gprof(output: str, top_n: int) -> List[Tuple[str, float]]:
    lines = output.splitlines()
    flat_idx = None
    for i, line in enumerate(lines):
        if line.strip().lower().startswith("flat profile"):
            flat_idx = i
            break
    if flat_idx is None:
        return []
    entries: List[Tuple[str, float]] = []
    header_seen = False
    for line in lines[flat_idx + 1 :]:
        line = line.strip()
        if not line:
            if header_seen:
                break
            continue
        if line.startswith("Each sample counts"):
            header_seen = True
            continue
        if line.startswith("%"):
            header_seen = True
            continue
        if not header_seen:
            continue
        parts = line.split()
        if len(parts) < 1:
            continue
        try:
            pct = float(parts[0])
            name = parts[-1]
            entries.append((name, pct))
            if len(entries) >= top_n:
                break
        except ValueError:
            continue
    return entries


def dataset_metadata(dataset_path: Path) -> Dict[str, float]:
    # One dataset == many frames. Metadata gives us scaling context for charts
    # (pixels, bytes, frame count) without re-reading during every experiment.
    pgm_files = sorted(dataset_path.glob("*.pgm"))
    if not pgm_files:
        raise RuntimeError(f"No .pgm files found in {dataset_path}")
    width, height, maxval = read_pgm_metadata(pgm_files[0])
    frames = len(pgm_files)
    pixels = width * height * frames
    bytes_per_pixel = 1 if maxval < 256 else 2
    total_bytes = pixels * bytes_per_pixel
    return {
        "width": width,
        "height": height,
        "frames": frames,
        "pixels": pixels,
        "bytes": total_bytes,
    }


def select_best_device(cpu_stats: Dict, gpu_stats: Optional[Dict]) -> Tuple[str, Dict]:
    cpu_time = best_stat(cpu_stats.get("stats") if cpu_stats else {})
    gpu_time = best_stat(gpu_stats.get("stats") if gpu_stats else {}) if gpu_stats else None
    if gpu_time is None and cpu_time is None:
        return "unavailable", {}
    if gpu_time is not None and (cpu_time is None or gpu_time <= cpu_time):
        return "gpu", gpu_stats
    return "cpu", cpu_stats


def compute_speedup(baseline: Optional[float], target: Optional[float]) -> Optional[float]:
    if baseline is None or target is None or target == 0:
        return None
    return baseline / target


def aggregate_best_times(dataset_results: Dict[VersionName, Dict]) -> Dict[VersionName, Dict]:
    best = {}
    for version, devices in dataset_results.items():
        best_device, best_entry = select_best_device(devices.get("cpu"), devices.get("gpu"))
        best[version] = {
            "device": best_device,
            "entry": best_entry,
            "time": best_stat(best_entry.get("stats", {})) if best_entry else None,
        }
    return best


def compute_execution_breakdown(entry: Optional[Dict]) -> Optional[Dict[str, float]]:
    if not entry:
        return None
    stats = entry.get("stats")
    if not stats:
        return None
    return {
        "computation": stats.get("user"),
        "memory_transfer": stats.get("sys"),
        "other": stats.get("other"),
    }


def compute_weak_scaling(best_times: Dict[str, Dict], metas: Dict[str, Dict]) -> Dict[str, List]:
    ordered = sorted(best_times.items(), key=lambda kv: metas[kv[0]]["pixels"])
    problem_sizes = [f"{metas[name]['width']}x{metas[name]['height']}x{metas[name]['frames']}" for name, _ in ordered]
    times = [info["time"] for _, info in ordered]
    base_time = times[0] if times and times[0] else None
    speedup = [compute_speedup(base_time, t) for t in times]
    result = {
        "problem_sizes": problem_sizes,
        "execution_time": times,
        "speedup": speedup,
    }
    log(f"Weak scaling result: {result}")
    return result


def compute_gpu_metrics(reference_dataset: str, datasets: Dict, versions: Sequence[VersionName], metas: Dict[str, Dict]) -> Dict:
    # GPU utilization charts need relative metrics (occupancy, bandwidth, SM use)
    # derived from simple throughput/bandwidth estimates. We normalize each
    # version against the best observed number so everything falls in [0,100].
    dataset_entry = datasets.get(reference_dataset)
    if not dataset_entry:
        log("Reference dataset missing for GPU metrics")
        return {}
    results = dataset_entry["results"]
    metrics = {"versions": [], "metrics": {"occupancy": [], "memory_bw": [], "sm_efficiency": []}}
    throughputs = {}
    bandwidths = {}
    for version in versions:
        gpu_entry = results.get(version, {}).get("gpu")
        if not gpu_entry:
            continue
        time_val = best_stat(gpu_entry.get("stats", {}))
        if not time_val or time_val <= 0:
            continue
        pixels = metas[reference_dataset]["pixels"]
        bytes_count = metas[reference_dataset]["bytes"]
        throughputs[version] = pixels / time_val
        bandwidths[version] = bytes_count / time_val
    if not throughputs:
        log("No GPU throughput data available")
        return {}
    max_throughput = max(throughputs.values())
    max_bw = max(bandwidths.values())
    for version in versions:
        if version not in throughputs:
            continue
        metrics["versions"].append(version)
        occupancy = (throughputs[version] / max_throughput) * 100.0 if max_throughput else 0.0
        mem_util = (bandwidths[version] / max_bw) * 100.0 if max_bw else 0.0
        speedups = []
        for other in throughputs.values():
            if other:
                speedups.append(throughputs[version] / other)
        sm_eff = statistics.mean(speedups) * 100.0 if speedups else 0.0
        metrics["metrics"]["occupancy"].append(occupancy)
        metrics["metrics"]["memory_bw"].append(mem_util)
        metrics["metrics"]["sm_efficiency"].append(sm_eff)
    log(f"GPU metrics: {metrics}")
    return metrics


def compute_roofline(reference_dataset: str, best_times: Dict[str, Dict], metas: Dict[str, Dict]) -> Dict:
    # Build classic roofline data: arithmetic intensity stays fixed per dataset,
    # so each version becomes a point in intensity/performance space. We also
    # capture the current "machine peak" (fastest observed) for comparison.
    dataset_meta = metas[reference_dataset]
    points = []
    performances = []
    bandwidths = []
    for version, info in best_times.items():
        t = info.get("time")
        if not t:
            continue
        intensity = dataset_meta["pixels"] / dataset_meta["bytes"]
        performance = dataset_meta["pixels"] / t
        bandwidth = dataset_meta["bytes"] / t
        points.append({"label": version, "intensity": intensity, "performance": performance})
        performances.append(performance)
        bandwidths.append(bandwidth)
    if not points:
        log("No roofline points generated")
        return {}
    machine_peak = max(performances) * 1.1
    bandwidth_bound = max(bandwidths) * 1.1
    roof = {"points": points, "machine_peak": machine_peak, "bandwidth_bound": bandwidth_bound}
    log(f"Roofline data: {roof}")
    return roof


def compute_efficiency(roofline: Dict, points: List[Dict]) -> Dict:
    if not roofline:
        log("Roofline input missing for efficiency computation")
        return {}
    peak = roofline.get("machine_peak")
    if not peak:
        log("Machine peak missing in roofline data")
        return {}
    versions = []
    percent = []
    for point in points:
        versions.append(point["label"])
        percent.append((point["performance"] / peak) * 100.0 if peak else 0.0)
    eff = {"versions": versions, "percent_peak": percent}
    log(f"Efficiency data: {eff}")
    return eff


def compute_memory_access_metrics(reference_dataset: str, datasets: Dict, metas: Dict[str, Dict]) -> Dict:
    # Lightweight proxy for how memory hierarchy tweaks affect latency vs. raw
    # throughput. Each strategy label maps to a version (global/shared/texture).
    mapping = {"V2": "Global", "V3": "Shared", "V4": "Texture"}
    dataset_entry = datasets.get(reference_dataset)
    if not dataset_entry:
        log("Reference dataset missing for memory access metrics")
        return {}
    strategies = []
    latency = []
    throughput = []
    for version, label in mapping.items():
        entry = dataset_entry["results"].get(version, {}).get("gpu")
        if not entry:
            continue
        t = best_stat(entry.get("stats", {}))
        if not t:
            continue
        pixels = metas[reference_dataset]["pixels"]
        bytes_count = metas[reference_dataset]["bytes"]
        strategies.append(label)
        latency.append(t / pixels)
        throughput.append(bytes_count / t)
    if not strategies:
        log("No strategies found for memory metrics")
        return {}
    metrics = {"strategies": strategies, "latency": latency, "throughput": throughput}
    log(f"Memory access metrics: {metrics}")
    return metrics


def collect_strong_scaling(
    dataset_path: Path,
    versions: Sequence[VersionName],
    version_dirs: Dict[VersionName, Path],
    block_sizes: Sequence[int],
    iterations: int,
) -> Dict:
    # For each version, sweep through several CUDA block sizes by setting
    # KLT_THREADS_PER_BLOCK and measure how runtime changes. This isolates how
    # the kernel reacts to different launch configurations.
    scaling = {"dataset": dataset_path.name, "block_sizes": list(block_sizes), "results": {}}
    for version in versions:
        version_dir = version_dirs[version]
        gpu_binary = version_dir / "example3_gpu"
        if not gpu_binary.exists():
            continue
        version_results = []
        for block in block_sizes:
            env = {"KLT_THREADS_PER_BLOCK": str(block)}
            run_info = collect_runs(gpu_binary, dataset_path, iterations, env)
            version_results.append(
                {
                    "block": block,
                    "time": best_stat(run_info.get("stats", {})),
                }
            )
        scaling["results"][version] = version_results
    # Compute per-version speedup relative to first block size
    for version, entries in scaling["results"].items():
        if not entries:
            continue
        baseline = entries[0]["time"]
        for entry in entries:
            entry["speedup"] = compute_speedup(baseline, entry["time"])
    log(f"Strong scaling results: {scaling}")
    return scaling


def compute_speedup_table(datasets: Dict, metas: Dict) -> None:
    # Augment each dataset entry with derived statistics so downstream plotting
    # code can access "best time per version" and speedup data in one place.
    for dataset, entry in datasets.items():
        results = entry["results"]
        best_times = aggregate_best_times(results)
        baseline = best_times.get("V1", {}).get("time")
        entry["derived"] = entry.get("derived", {})
        entry["derived"]["best_times"] = best_times
        entry["derived"]["speedup"] = {
            version: compute_speedup(baseline, info.get("time"))
            for version, info in best_times.items()
        }
        entry["derived"]["breakdown"] = {
            version: compute_execution_breakdown(info.get("entry"))
            for version, info in best_times.items()
        }
        log(f"Dataset {dataset} derived data: {entry['derived']}")


def main() -> None:
    args = parse_args()
    src_root = args.src_root.resolve()
    dataset_root = (src_root / args.dataset_root).resolve()
    version_dirs = {v: (src_root / v) for v in args.versions}
    datasets = discover_datasets(dataset_root)
    metas = {name: dataset_metadata(path) for name, path in datasets.items()}

    # All measurements share one JSON blob: build targets, run benchmarks,
    # compute derived metrics, then dump to disk for the plotting notebooks.
    data = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "iterations": args.iterations,
        "datasets": {},
    }

    for version, vdir in version_dirs.items():
        if not vdir.exists():
            raise FileNotFoundError(f"Version directory {vdir} missing")
        if not args.dry_run:
            success, output = build_target(vdir, "example3")
            if not success:
                raise RuntimeError(f"{version} CPU build failed:\n{trim_output(output)}")
        if (vdir / "convolve_gpu.cu").exists():
            if not args.dry_run:
                success, output = build_target(vdir, "example3_gpu")
                if not success:
                    raise RuntimeError(f"{version} GPU build failed:\n{trim_output(output)}")
        else:
            log(f"[warn] No GPU sources found for {version}; GPU runs will be skipped")

    for dset_name, dset_path in datasets.items():
        dataset_entry = {"meta": metas[dset_name], "results": {}, "path": str(dset_path)}
        for version, vdir in version_dirs.items():
            results = {}
            cpu_binary = vdir / "example3"
            if cpu_binary.exists() and not args.dry_run:
                cpu_info = collect_runs(cpu_binary, dset_path, args.iterations)
            else:
                cpu_info = None
            results["cpu"] = cpu_info
            gpu_binary = vdir / "example3_gpu"
            if gpu_binary.exists() and not args.dry_run:
                gpu_info = collect_runs(gpu_binary, dset_path, args.iterations)
            else:
                gpu_info = None
            results["gpu"] = gpu_info
            dataset_entry["results"][version] = results
        data["datasets"][dset_name] = dataset_entry

    compute_speedup_table(data["datasets"], metas)

    reference_dataset_path = datasets.get(args.reference_dataset)
    if reference_dataset_path and not args.dry_run:
        data["strong_scaling"] = collect_strong_scaling(
            reference_dataset_path,
            [v for v in args.versions if (version_dirs[v] / "example3_gpu").exists()],
            version_dirs,
            args.strong_scaling_blocks,
            max(1, args.strong_scaling_iterations),
        )
    else:
        data["strong_scaling"] = {}

    if not args.dry_run:
        hotspot_entries = {}
        for version in args.profile_versions:
            if version not in version_dirs:
                continue
            points = collect_hotspots(version_dirs[version], reference_dataset_path or next(iter(datasets.values())))
            hotspot_entries[version] = points
        functions = set()
        for entries in hotspot_entries.values():
            for name, _ in entries:
                functions.add(name)
        functions = list(functions)
        hotspot = {"functions": functions}
        for version, entries in hotspot_entries.items():
            percentages = []
            for func in functions:
                pct = next((pct for (name, pct) in entries if name == func), 0.0)
                percentages.append(pct)
            hotspot[version] = percentages
        data["hotspot_analysis"] = hotspot
    else:
        data["hotspot_analysis"] = {}

    best_times_reference = {
        name: aggregate_best_times(entry["results"])
        for name, entry in data["datasets"].items()
    }
    weak_scaling_source = {
        name: info["derived"]["best_times"]["V4"]
        if "derived" in info and "V4" in info["derived"]["best_times"]
        else aggregate_best_times(info["results"]).get("V4", {})
        for name, info in data["datasets"].items()
    }
    weak_best_times = {name: weak_scaling_source[name] for name in data["datasets"].keys()}
    data["weak_scaling"] = compute_weak_scaling(
        {name: weak_best_times[name] for name in data["datasets"]},
        metas,
    )

    reference_best = best_times_reference.get(args.reference_dataset, {})
    data["gpu_utilization"] = compute_gpu_metrics(
        args.reference_dataset,
        data["datasets"],
        [v for v in args.versions if v != "V1"],
        metas,
    )
    roofline = compute_roofline(
        args.reference_dataset,
        reference_best,
        metas,
    )
    data["roofline"] = roofline
    points = roofline.get("points", []) if roofline else []
    data["efficiency"] = compute_efficiency(roofline, points)
    data["memory_access"] = compute_memory_access_metrics(
        args.reference_dataset, data["datasets"], metas
    )

    output_path = (src_root / args.output).resolve()
    if args.dry_run:
        log(f"[dry-run] Skipping write to {output_path}")
    else:
        with output_path.open("w", encoding="utf-8") as fh:
            json.dump(data, fh, indent=2)
        log(f"Wrote benchmark data to {output_path}")


if __name__ == "__main__":
    main()
